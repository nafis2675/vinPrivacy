# -*- coding: utf-8 -*-
"""cautionPlate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FgwFUf4R_htn8IQWO5SKs0ue1a1Zt2Kc
"""

#@title 1. Install PaddleOCR and Dependencies
# ----- Installation -----
# Install paddlepaddle (CPU or GPU version). PaddleOCR will attempt to use GPU if available.
# We install the CPU version here for broader compatibility, but you can uncomment
# the GPU version if you know you have a GPU runtime in Colab (Runtime -> Change runtime type -> GPU).
!pip install paddlepaddle -q -U
# !pip install paddlepaddle-gpu -q -U # Uncomment for GPU

# Install PaddleOCR
!pip install paddleocr -q -U

# Install a font for visualization (avoids potential errors with draw_ocr)
!apt-get install -y fonts-wqy-zenhei
print("Installation Complete!")

#@title 2. Import Libraries
# ----- Imports -----
import os
import cv2 # OpenCV for image handling
from google.colab import files
from paddleocr import PaddleOCR, draw_ocr # Main PaddleOCR class and drawing utility
import matplotlib.pyplot as plt # For displaying images
from PIL import Image # Python Imaging Library

# Suppress logging messages from PaddleOCR
import logging
logging.disable(logging.INFO)

print("Libraries Imported!")

# Define font path for visualization (installed in the previous step)
# Using a common path where apt installs fonts in Colab/Debian systems
font_path = '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc'

#@title 3. Initialize PaddleOCR Engine
# ----- Initialize OCR -----
# You can change the language parameter ('lang') if needed.
# Examples: 'en', 'ch', 'fr', 'german', 'korean', 'japan'
# For multiple languages: lang='en+ch'
# use_angle_cls=True helps detect text orientation.
# use_gpu=True/False explicitly sets GPU/CPU usage (PaddleOCR usually auto-detects)

selected_language = 'en' #@param {type:"string"} ['en', 'ch', 'fr', 'de', 'ko', 'ja', 'en+ch', 'en+fr']

print(f"Initializing PaddleOCR with language: '{selected_language}'...")
# Check if GPU is available and paddlepaddle-gpu is installed
try:
    import paddle
    gpu_available = paddle.is_compiled_with_cuda()
    if gpu_available:
        print("GPU detected. Attempting to use GPU.")
        ocr_engine = PaddleOCR(use_angle_cls=True, lang=selected_language, use_gpu=True, show_log=False)
    else:
        print("GPU not detected or paddlepaddle-gpu not installed. Using CPU.")
        ocr_engine = PaddleOCR(use_angle_cls=True, lang=selected_language, use_gpu=False, show_log=False)
except Exception as e:
    print(f"Error checking GPU or initializing: {e}. Defaulting to CPU.")
    ocr_engine = PaddleOCR(use_angle_cls=True, lang=selected_language, use_gpu=False, show_log=False)

print("PaddleOCR Engine Initialized!")

#@title 4. Upload Image
# ----- Upload Image -----
uploaded = files.upload()

if not uploaded:
  print("\nNo file uploaded. Please run this cell again and select a file.")
  # Raise an error to stop execution if you prefer
  # raise ValueError("No file uploaded.")
else:
  # Get the filename of the first uploaded file
  img_path = list(uploaded.keys())[0]
  print(f"\nUploaded '{img_path}' ({len(uploaded[img_path])} bytes)")

  # Display the uploaded image (optional)
  print("Displaying uploaded image:")
  display(Image.open(img_path))

#@title 5. Run OCR and Display Results
# ----- Run OCR and Show Results -----

if 'img_path' not in locals() or not os.path.exists(img_path):
  print("Image path not found. Please upload an image in the previous step first.")
else:
  print(f"\nRunning OCR on '{img_path}'...")

  # Perform OCR
  # The result is a list of lists, where each inner list contains
  # [bounding_box, (text, confidence_score)]
  # e.g., [[[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], ('Detected Text', 0.99)]]
  # Sometimes the result is nested one level deeper, like result[0] = [...]
  result = ocr_engine.ocr(img_path, cls=True)

  # Handle potential nesting
  if result and isinstance(result[0], list) and len(result) == 1:
      ocr_results = result[0]
  else:
      ocr_results = result # Assume it's already in the correct format

  print("\n--- Detected Text ---")
  if not ocr_results:
      print("No text detected.")
  else:
      # Extract text, boxes, and scores
      boxes = [line[0] for line in ocr_results]
      texts = [line[1][0] for line in ocr_results]
      scores = [line[1][1] for line in ocr_results]

      # Print extracted text with scores
      for i, (text, score) in enumerate(zip(texts, scores)):
          print(f"{i+1}. {text} (Confidence: {score:.4f})")

      print("\n--- Visualization ---")
      # Load image with OpenCV for drawing
      image = cv2.imread(img_path)

      # Draw the OCR results on the image
      # Make sure the font_path is correct, otherwise drawing might fail or use a default font
      try:
          image_with_boxes = draw_ocr(image, boxes, texts, scores, font_path=font_path)

          # Convert from BGR (OpenCV default) to RGB (Matplotlib default)
          image_rgb = cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB)

          # Display the image with bounding boxes
          plt.figure(figsize=(15, 10)) # Adjust figure size as needed
          plt.imshow(image_rgb)
          plt.title(f'PaddleOCR Results for {os.path.basename(img_path)}')
          plt.axis('off') # Hide axes
          plt.show()

      except Exception as e:
          print(f"\nError during visualization: {e}")
          print("This might be due to font issues or image loading problems.")
          print("Displaying the original image instead:")
          display(Image.open(img_path))


  # Optional: Clean up the uploaded file
  # os.remove(img_path)
  # print(f"Cleaned up uploaded file: {img_path}")

#@title 6. Find and Obscure Chassis/VIN/Frame Number (Handles Merged & Split IDs)

import re
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import math
import os # Ensure os is imported

# --- Configuration ---
# Keywords that might *prefix* or be near an identifier (case-insensitive)
KEYWORDS = ['vin', 'chassis', 'frame no', 'frameno', 'frame', 'serial', 'vehicle id', 'identifica', 'chassisno', 'chassis no'] # Added more specific variants

# Regular Expression for a standard 17-character VIN
VIN_PATTERN = re.compile(r'\b[A-HJ-NPR-Z0-9]{17}\b', re.IGNORECASE)

# Pattern for a potential identifier PART (used after keyword or for continuation)
# Allows letters, numbers, hyphens, minimum length 1 (more flexible)
# Ensures it doesn't ONLY consist of characters we strip (like just '-')
IDENTIFIER_PART_PATTERN = re.compile(r'^(?=.*[A-Z0-9])[A-Z0-9-]+$', re.IGNORECASE)

# Pattern to check if a block looks like a plausible CONTINUATION of an ID (often numeric or simple alphanumeric)
CONTINUATION_PATTERN = re.compile(r'^[A-Z0-9-]+$', re.IGNORECASE) # Simple check: letters, numbers, hyphen

# Characters to potentially strip after a keyword
STRIP_CHARS = ' .:-'

# --- User Parameters ---
obscure_part = 'Last' #@param ["First", "Middle", "Last", "Full"] {allow-input: false}
obscure_method = 'Blur' #@param ["Blur", "Mosaic"] {allow-input: false}
blur_amount = 100 #@param {type:"slider", min:1, max:51, step=2}
mosaic_block_size = 10 #@param {type:"slider", min:2, max:30, step=1}

# --- Helper Function ---
def merge_boxes(box1, box2):
    """ Calculates the bounding box encompassing two PaddleOCR boxes """
    points1 = np.array(box1, dtype=np.int32)
    points2 = np.array(box2, dtype=np.int32)
    all_points = np.vstack((points1, points2))
    x, y, w, h = cv2.boundingRect(all_points)
    # Return in PaddleOCR corner format (approximate)
    # return [[x, y], [x + w, y], [x + w, y + h], [x, y + h]]
    # Return as OpenCV rect [x, y, w, h] for easier processing later
    return [x, y, w, h]

# --- Processing ---

# Ensure prerequisite variables exist
if 'ocr_results' not in locals() or not ocr_results:
    print("‚ùå Error: OCR results not found or empty. Please run Cell #5 first.")
elif 'img_path' not in locals() or not os.path.exists(img_path):
     print(f"‚ùå Error: Original image path '{img_path}' not found or file missing. Please run Cell #4 (Upload) and Cell #5 (OCR) again.")
else:
    print(f"üîé Searching for VIN/Chassis/Frame numbers...")
    print(f"   Obscuring method: {obscure_method}, Part: {obscure_part}")

    # Load the original image
    image = cv2.imread(img_path)
    if image is None:
        print(f"‚ùå Error: Could not load image file '{img_path}'.")
    else:
        output_image = image.copy() # Work on a copy
        found_ids = [] # Store found identifiers {'full_text': ..., 'id_part': ..., 'box_rect': [x,y,w,h], 'method': ...}
        processed_indices = set() # Keep track of indices used in merges

        # Store texts and their bounding boxes [(text, box), ...]
        texts_and_boxes = [(line[1][0], line[0]) for line in ocr_results]
        num_blocks = len(texts_and_boxes)

        for i in range(num_blocks):
            if i in processed_indices:
                continue # Skip if this block was already merged with a previous one

            text, box = texts_and_boxes[i]
            text_lower = text.lower()
            match_found = False

            # 1. Check for standard 17-digit VIN pattern anywhere in the string
            vin_match = VIN_PATTERN.search(text)
            if vin_match:
                matched_id_string = vin_match.group(0)
                print(f"  ‚úÖ Found by VIN Pattern: '{matched_id_string}' in block {i}: '{text}'")
                pts = np.array(box, dtype=np.int32)
                rect = cv2.boundingRect(pts) # [x, y, w, h]
                found_ids.append({'full_text': text, 'id_part': matched_id_string, 'box_rect': list(rect), 'method': 'Pattern'})
                processed_indices.add(i)
                match_found = True
                continue # Pattern match takes precedence

            # 2. Check if the block STARTS WITH a keyword
            if not match_found:
                for keyword in KEYWORDS:
                    if text_lower.startswith(keyword):
                        # Found a keyword prefix
                        potential_id_part1 = text[len(keyword):].strip(STRIP_CHARS)

                        # Check if the rest of this block looks like an identifier PART
                        is_part1_valid = IDENTIFIER_PART_PATTERN.match(potential_id_part1)

                        # --- Check for Continuation in the NEXT block (i+1) ---
                        if i + 1 < num_blocks and (i + 1) not in processed_indices:
                            next_text, next_box = texts_and_boxes[i+1]
                            # Check if next block looks like a continuation
                            is_continuation = CONTINUATION_PATTERN.match(next_text)

                            if is_continuation and (is_part1_valid or not potential_id_part1): # Allow combining even if part1 was empty/invalid chars after keyword
                                combined_id = (potential_id_part1 + next_text).strip() # Combine parts
                                merged_rect = merge_boxes(box, next_box) # Merge bounding boxes
                                full_combined_text = f"{text} | {next_text}" # For logging

                                print(f"  ‚úÖ Found by Keyword Prefix '{keyword}' & Continuation: Combined ID '{combined_id}' from blocks {i} ('{text}') and {i+1} ('{next_text}')")
                                found_ids.append({'full_text': full_combined_text, 'id_part': combined_id, 'box_rect': merged_rect, 'method': 'Keyword Prefix + Continuation'})
                                processed_indices.add(i)
                                processed_indices.add(i+1) # Mark next block as processed too
                                match_found = True
                                break # Keyword found and processed (potentially with merge)

                        # --- If NO continuation, check if part1 alone is a valid ID ---
                        if not match_found and is_part1_valid:
                            print(f"  ‚úÖ Found by Keyword Prefix '{keyword}': ID '{potential_id_part1}' in block {i}: '{text}' (No continuation found/used)")
                            pts = np.array(box, dtype=np.int32)
                            rect = cv2.boundingRect(pts)
                            found_ids.append({'full_text': text, 'id_part': potential_id_part1, 'box_rect': list(rect), 'method': 'Keyword Prefix Only'})
                            processed_indices.add(i)
                            match_found = True
                            break # Keyword found and processed

                    if match_found: break # Exit keyword loop if match found for this block


        # --- Apply Obscuring ---
        if not found_ids:
            print("\n‚ùå No potential identifiers found matching the criteria.")
            # Display the original image again if nothing was found
            plt.figure(figsize=(15, 10))
            plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))
            plt.title(f'Original Image - No Identifier Found/Obscured')
            plt.axis('off')
            plt.show()
        else:
            print(f"\nüëç Found {len(found_ids)} potential identifier(s). Applying obscuring...")

            for id_info in found_ids:
                x, y, w, h = id_info['box_rect'] # Use the (potentially merged) bounding rect
                full_text = id_info['full_text'] # Original text (or combined text for merges)
                id_part = id_info['id_part']     # The actual ID string extracted
                method = id_info['method']

                # Ensure coordinates are within image bounds (should be handled by cv2.boundingRect already, but double check)
                x1, y1 = max(0, x), max(0, y)
                x2, y2 = min(output_image.shape[1], x + w), min(output_image.shape[0], y + h)

                # Recalculate width and height based on final clipped coords
                final_w = x2 - x1
                final_h = y2 - y1

                if final_w <= 0 or final_h <= 0:
                    print(f"  ‚ö†Ô∏è Skipping ID '{id_part}' (from '{full_text}') due to zero size rect: [{x1},{y1},{final_w},{final_h}]. Method: {method}")
                    continue

                # --- Calculate the ROI to obscure based on user choice ---
                obscure_x1, obscure_y1 = x1, y1
                obscure_x2, obscure_y2 = x2, y2

                if obscure_part == "First":
                    obscure_x2 = x1 + final_w // 3
                elif obscure_part == "Middle":
                    obscure_x1 = x1 + final_w // 3
                    obscure_x2 = x1 + (2 * final_w) // 3
                elif obscure_part == "Last":
                    obscure_x1 = x1 + (2 * final_w) // 3
                # else: "Full" uses the default x1, y1, x2, y2

                # Ensure region has size (int conversion)
                obscure_x1, obscure_y1 = int(obscure_x1), int(obscure_y1)
                obscure_x2, obscure_y2 = int(obscure_x2), int(obscure_y2)

                if obscure_x1 >= obscure_x2 or obscure_y1 >= obscure_y2:
                    print(f"  ‚ö†Ô∏è Calculated obscuring region for ID '{id_part}' has zero size. Defaulting to Full.")
                    obscure_x1, obscure_y1, obscure_x2, obscure_y2 = x1, y1, x2, y2

                # Extract the ROI (Region of Interest)
                roi = output_image[obscure_y1:obscure_y2, obscure_x1:obscure_x2]

                if roi.size == 0:
                     print(f"  ‚ö†Ô∏è Skipping obscuring for ID '{id_part}' because ROI is empty ({obscure_y1}:{obscure_y2}, {obscure_x1}:{obscure_x2}). Box rect was [{x},{y},{w},{h}]")
                     continue

                print(f"  ‚öôÔ∏è Obscuring '{obscure_part}' part of ID '{id_part}' (Method: {method}) at rect [{obscure_x1},{obscure_y1} -> {obscure_x2},{obscure_y2}] using {obscure_method}")

                # Apply the chosen obscuring method
                try:
                    if obscure_method == 'Blur':
                        ksize = blur_amount if blur_amount % 2 != 0 else blur_amount + 1
                        blurred_roi = cv2.GaussianBlur(roi, (ksize, ksize), 0)
                        output_image[obscure_y1:obscure_y2, obscure_x1:obscure_x2] = blurred_roi
                    elif obscure_method == 'Mosaic':
                        roi_h, roi_w = roi.shape[:2]
                        temp_block_size_w = max(2, min(mosaic_block_size, roi_w // 2 if roi_w > 1 else 2))
                        temp_block_size_h = max(2, min(mosaic_block_size, roi_h // 2 if roi_h > 1 else 2))
                        target_w = max(1, roi_w // temp_block_size_w)
                        target_h = max(1, roi_h // temp_block_size_h)
                        small_roi = cv2.resize(roi, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
                        mosaic_roi = cv2.resize(small_roi, (roi_w, roi_h), interpolation=cv2.INTER_NEAREST)
                        output_image[obscure_y1:obscure_y2, obscure_x1:obscure_x2] = mosaic_roi
                except Exception as e:
                     print(f"  ‚ùå Error applying {obscure_method} to ROI for ID '{id_part}': {e}")


            # Display the final image
            print("\nüñºÔ∏è --- Obscured Image ---")
            plt.figure(figsize=(15, 10))
            plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))
            plt.title(f'Image with Obscured Identifiers (Method: {obscure_method}, Part: {obscure_part})')
            plt.axis('off')
            plt.show()